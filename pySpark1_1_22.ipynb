{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This implementation performs the following changes:\n",
        "\n",
        "1. Reciprocal Transformation to normalize variables (w/nulls Dropped)\n",
        "\n"
      ],
      "metadata": {
        "id": "VdCaSxMhuID8"
      },
      "id": "VdCaSxMhuID8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc88b9ad-6ed7-462b-b241-048cab574a05",
      "metadata": {
        "id": "fc88b9ad-6ed7-462b-b241-048cab574a05"
      },
      "outputs": [],
      "source": [
        "!pip install numpy\n",
        "!pip install pyspark\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d6553129-3496-4321-a895-9b0a3311cd4c",
      "metadata": {
        "id": "d6553129-3496-4321-a895-9b0a3311cd4c"
      },
      "outputs": [],
      "source": [
        "#Basic Imports\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#ML Classifier Imports\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import OneVsRest\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import when\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import sqrt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "033bc9a6-1ff9-45d5-b6dd-eba9c159897d",
      "metadata": {
        "id": "033bc9a6-1ff9-45d5-b6dd-eba9c159897d"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"rka7\") \\\n",
        "    .master(\"local\") \\\n",
        "    .config(\"spark.driver.cores\", \"5\") \\\n",
        "    .config(\"spark.driver.memory\", \"10g\") \\\n",
        "    .config(\"spark.executor.memory\", \"5g\") \\\n",
        "    .config(\"spark.executor.cores\", \"4\") \\\n",
        "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", \"true\") \\\n",
        "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
        "    .config(\"spark.dynamicAllocation.minExecutors\", \"2\") \\\n",
        "    .config(\"spark.dynamicAllocation.maxExecutors\", \"4\") \\\n",
        "    .config(\"spark.executor.instances\", \"2\") \\\n",
        ".getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "75273b9a-33d4-4f17-b57c-44ee1fdcfda9",
      "metadata": {
        "id": "75273b9a-33d4-4f17-b57c-44ee1fdcfda9"
      },
      "outputs": [],
      "source": [
        "# Get the parquet files (current example is 2 from the website)\n",
        "parquet_files = [\"/content/part-00000-23fdcfa3-9dd3-4c72-886c-e945bfcf92e1-c000.snappy.parquet\", \\\n",
        "                 \"/content/part-00000-9aeb279c-81c6-4481-9b30-d35d4d194fea-c000.snappy.parquet\"]\n",
        "# Read the parquet files into a dataframe\n",
        "df = spark.read.parquet(*parquet_files, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# All service column is NULL\n",
        "df = df.drop(\"service\")\n",
        "\n",
        "# Remove instances with null values\n",
        "df = df.dropna()\n",
        "\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "VZwf40RC5ab6"
      },
      "id": "VZwf40RC5ab6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert timestamp column to string\n",
        "df = df.withColumn(\"datetime_str\", col(\"datetime\").cast(\"string\"))\n",
        "\n",
        "#Drop the datetime column\n",
        "df = df.drop(\"datetime\")\n",
        "\n",
        "# Define columns to index\n",
        "columns_to_index = ['conn_state', 'history', 'proto', 'dest_ip_zeek', 'community_id', 'uid', 'src_ip_zeek', 'label_tactic', \\\n",
        "                    'label_binary', 'label_technique', 'datetime_str']\n",
        "\n",
        "# Apply StringIndexer to each column\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_indexed\").fit(df) for column in columns_to_index]\n",
        "\n",
        "# Chain indexers together\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "\n",
        "# Fit and transform the data\n",
        "df_indexed = pipeline.fit(df).transform(df)\n",
        "\n",
        "# Drop original columns\n",
        "df_indexed = df_indexed.drop(*columns_to_index)\n",
        "\n",
        "# Drop rows with any null values\n",
        "df_indexed = df_indexed.dropna()\n",
        "\n",
        "# Show the schema of the DataFrame\n",
        "#df_indexed.printSchema()\n",
        "#df.select(\"label_tactic\").distinct().collect()"
      ],
      "metadata": {
        "id": "HsgH0C-vCHq1"
      },
      "id": "HsgH0C-vCHq1",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_assembled = df_assembled.drop(\"datetime_str\")"
      ],
      "metadata": {
        "id": "vhnDU8vQemTg"
      },
      "id": "vhnDU8vQemTg",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of columns to assemble\n",
        "columns_to_assemble = df_indexed.columns\n",
        "\n",
        "# Defining Numerical columns to Root Transform\n",
        "transform_columns = ['resp_pkts', 'orig_ip_bytes', 'missed_bytes', 'duration', 'orig_pkts', \\\n",
        "               'dest_port_zeek', 'orig_bytes', 'resp_bytes', 'src_port_zeek']\n",
        "\n",
        "# Perform Reciprocal Transformation to normalize Numerical columns\n",
        "for column in transform_columns:\n",
        "    df = df.withColumn(column, 1 / col(column))\n",
        "\n",
        "# Numerical columns to add to Assembler\n",
        "num_columns = ['resp_pkts', 'orig_ip_bytes', 'missed_bytes', 'duration', 'orig_pkts', \\\n",
        "               'dest_port_zeek', 'orig_bytes', 'local_orig', 'resp_bytes', 'src_port_zeek', 'local_resp', 'resp_ip_bytes', 'ts']\n",
        "\n",
        "# Include Numerical columns to be assembled (Problems: 'local_resp', 'resp_ip_bytes', 'ts' is causing evaluations to overfit?)\n",
        "columns_to_assemble.extend(num_columns)\n",
        "\n",
        "# Remove the target column (label) if it's in the list\n",
        "columns_to_assemble.remove('label_tactic_indexed')\n",
        "\n",
        "# Create the VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=columns_to_assemble, outputCol=\"features\")\n",
        "\n",
        "# Transform the DataFrame\n",
        "df_assembled = assembler.transform(df_indexed)\n",
        "\n",
        "# Select only the features and label columns\n",
        "df_assembled = df_assembled.select(\"features\", \"label_tactic_indexed\")\n",
        "\n",
        "# Show the schema of the DataFrame\n",
        "#df_assembled.printSchema()"
      ],
      "metadata": {
        "id": "XPtNBmnUCM3s"
      },
      "id": "XPtNBmnUCM3s",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "66981ef6-1c0b-42ce-965a-f60038c98d66",
      "metadata": {
        "id": "66981ef6-1c0b-42ce-965a-f60038c98d66"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and test sets\n",
        "train_data, test_data = df_assembled.randomSplit([0.8, 0.2], seed=1)\n",
        "\n",
        "# Create the SVM model\n",
        "svm = LinearSVC(labelCol=\"label_tactic_indexed\", featuresCol=\"features\", maxIter=10)\n",
        "\n",
        "# One Vs. Rest\n",
        "ovr = OneVsRest(classifier=svm, labelCol='label_tactic_indexed')\n",
        "\n",
        "# Fit the model\n",
        "svm_model = ovr.fit(train_data)\n",
        "\n",
        "# Make predictions\n",
        "predictions = svm_model.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Mulitple Evaluators for weighted precision, weighted recall, accuracy, weighted FPR\n",
        "#eval_precision = MulticlassClassificationEvaluator(labelCol=\"label_tactic_indexed\", metricName=\"weightedPrecision\")\n",
        "#eval_recall = MulticlassClassificationEvaluator(labelCol=\"label_tactic_indexed\", metricName=\"weightedRecall\")\n",
        "eval_accuracy = MulticlassClassificationEvaluator(labelCol=\"label_tactic_indexed\", metricName=\"accuracy\")\n",
        "#eval_fpr = MulticlassClassificationEvaluator(labelCol=\"label_tactic_indexed\", metricName=\"weightedFalsePositiveRate\")\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = eval_accuracy.evaluate(predictions)\n",
        "print(\"Accuracy:\", evaluator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W87jusSBx-gQ",
        "outputId": "e9d04968-f944-4b8e-c2d2-2b07bd03bea4"
      },
      "id": "W87jusSBx-gQ",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes made:\n",
        "\n",
        "Includes the Reciprocal Transformation on numerical variables."
      ],
      "metadata": {
        "id": "ISps2h1ztOPB"
      },
      "id": "ISps2h1ztOPB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes Noticed:\n",
        "\n",
        "Including indexed datetime into features lowers accuracy by .2"
      ],
      "metadata": {
        "id": "GWoN6TtoNEMR"
      },
      "id": "GWoN6TtoNEMR"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "35a12f58-f3b0-408c-904a-87018b432259",
      "metadata": {
        "id": "35a12f58-f3b0-408c-904a-87018b432259"
      },
      "outputs": [],
      "source": [
        "spark.sparkContext.stop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}