{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This implementation performs the following changes:\n",
        "1. Changes any instances of \"null\" or NaN for numbers to the mean value.\n",
        "2. Changes any null values for strings to 'null'\n",
        "3. Uses a StringIndexer on the following features to conver them to numerical data. The feature name is changed to +'_indexed' when using this method. a. 'service' b. 'conn_state' c. 'history' d. 'proto' e. 'dest_ip_zeek' f. 'community_id' g. 'uid' h. 'src_ip_zeek'\n",
        "4. The original feature columns (not _indexed) are removed.\n",
        "5. Uses a StringIndexer on the class labels, 'label_tactic', to convert them to numerical data.\n",
        "6. Uses a VectorAssembler on the new features.\n",
        "7. Performs PCA on the data. (k=3)\n",
        "7. Trains the SVM model using OVR.\n",
        "\n",
        "\n",
        "NOTE: There's an error that needs to be corrected: currently the PCA is done on all data PRIOR to splitting into training and test. This could lead to data-leakage. For now, this notebook serves as a \"proof-of-concept\"."
      ],
      "metadata": {
        "id": "FW3tf7tmBFdX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mFmyTA9BDCz"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Imports\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# ML Classifier Imports\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import OneVsRest\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "from pyspark.sql.functions import mean, col"
      ],
      "metadata": {
        "id": "f2aDKkm1CnSx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"ce53\") \\\n",
        "    .master(\"local\") \\\n",
        "    .config(\"spark.driver.cores\", \"4\") \\\n",
        "    .config(\"spark.driver.memory\", \"5g\") \\\n",
        "    .config(\"spark.executor.memory\", \"5g\") \\\n",
        "    .config(\"spark.executor.cores\", \"4\") \\\n",
        "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", \"true\") \\\n",
        "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
        "    .config(\"spark.dynamicAllocation.minExecutors\", \"2\") \\\n",
        "    .config(\"spark.dynamicAllocation.maxExecutors\", \"4\") \\\n",
        "    .config(\"spark.executor.instances\", \"2\") \\\n",
        ".getOrCreate()"
      ],
      "metadata": {
        "id": "lhELG0zyCsfo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the parquet files (current example is 2 from the website)\n",
        "parquet_files = [\"/content/part-00000-9aeb279c-81c6-4481-9b30-d35d4d194fea-c000.snappy.parquet\", \"/content/part-00000-ea53b0e8-d346-44e3-9a87-1f60ac35c610-c000.snappy.parquet\"]\n",
        "\n",
        "# Read the parquet files into a dataframe\n",
        "df = spark.read.parquet(*parquet_files, inferSchema=True)"
      ],
      "metadata": {
        "id": "03tmSx8VCt-9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "F9EWZom9Cv-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Rro3du1Tb4AG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mean, col\n",
        "\n",
        "# List of numeric column names\n",
        "numeric_columns = ['resp_pkts', 'orig_ip_bytes', 'missed_bytes', 'duration', 'orig_pkts',\n",
        "                   'resp_ip_bytes', 'dest_port_zeek', 'orig_bytes', 'resp_bytes',\n",
        "                   'src_port_zeek', 'ts']\n",
        "\n",
        "transform_columns = ['resp_pkts', 'orig_ip_bytes', 'missed_bytes', 'duration', 'orig_pkts', \\\n",
        "               'dest_port_zeek', 'orig_bytes', 'resp_bytes', 'src_port_zeek']\n",
        "\n",
        "# Calculate mean for each numeric column\n",
        "mean_values = df.select([mean(col(column)).alias(column) for column in numeric_columns]).collect()[0].asDict()\n",
        "\n",
        "# Replace null or NaN values with mean\n",
        "for column in numeric_columns:\n",
        "    mean_value = mean_values[column]\n",
        "    df = df.fillna({column: mean_value}, subset=[column])\n",
        "\n",
        "# Show updated DataFrame\n",
        "#df.show()"
      ],
      "metadata": {
        "id": "mRWfKxzPJEet"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert timestamp column to string\n",
        "df = df.withColumn(\"datetime_str\", col(\"datetime\").cast(\"string\"))\n",
        "\n",
        "# Drop the datetime column\n",
        "df = df.drop(\"datetime\")\n",
        "\n",
        "# Define columns to index\n",
        "columns_to_index = ['service', 'conn_state', 'history', 'proto', 'dest_ip_zeek', 'community_id', 'uid', 'src_ip_zeek', 'label_tactic', \\\n",
        "                    'label_technique', 'label_binary', 'datetime_str']\n",
        "\n",
        "# Impute null values with 'null' string\n",
        "for column in columns_to_index:\n",
        "    df = df.fillna('null', subset=[column])\n",
        "\n",
        "# Apply StringIndexer to each column\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_indexed\").fit(df) for column in columns_to_index]\n",
        "\n",
        "# Chain indexers together\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "\n",
        "# Fit and transform the data\n",
        "df_indexed = pipeline.fit(df).transform(df)\n",
        "\n",
        "# Drop original columns\n",
        "df_indexed = df_indexed.drop(*columns_to_index)\n",
        "\n",
        "# Show the schema of the DataFrame\n",
        "#df_indexed.show()"
      ],
      "metadata": {
        "id": "JoVneK9OJ6rH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for null values in each column\n",
        "#for column in df_indexed.columns:\n",
        "#    null_count = df_indexed.where(col(column).isNull()).count()\n",
        "#    print(f\"Null count in column {column}: {null_count}\")"
      ],
      "metadata": {
        "id": "kTHV-Q0-QEAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of columns to assemble\n",
        "columns_to_assemble = df_indexed.columns\n",
        "\n",
        "# Remove the target column (label) if it's in the list\n",
        "columns_to_assemble.remove('label_tactic_indexed')\n",
        "\n",
        "# Create the VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=columns_to_assemble, outputCol=\"features\")\n",
        "\n",
        "# Transform the DataFrame\n",
        "df_assembled = assembler.transform(df_indexed)\n",
        "\n",
        "# Select only the features and label columns\n",
        "df_assembled = df_assembled.select(\"features\", \"label_tactic_indexed\")\n",
        "\n",
        "# Show the schema of the DataFrame\n",
        "#df_assembled.printSchema()"
      ],
      "metadata": {
        "id": "_Sh9zrXHLSiN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import PCA\n",
        "\n",
        "# Define the PCA model\n",
        "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pca_features\")\n",
        "\n",
        "# Fit the PCA model\n",
        "pca_model = pca.fit(df_assembled)\n",
        "\n",
        "# Apply PCA transformation to the assembled DataFrame\n",
        "df_pca = pca_model.transform(df_assembled)\n",
        "\n",
        "# Show the transformed DataFrame\n",
        "#df_pca.show()"
      ],
      "metadata": {
        "id": "DpUcNL6idytP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test sets\n",
        "train_data, test_data = df_pca.randomSplit([0.8, 0.2], seed=1)\n",
        "\n",
        "# Create the SVM model\n",
        "svm = LinearSVC(labelCol=\"label_tactic_indexed\", featuresCol=\"features\", maxIter=10)\n",
        "\n",
        "# One Vs. Rest\n",
        "ovr = OneVsRest(classifier=svm, labelCol='label_tactic_indexed')\n",
        "\n",
        "# Fit the model\n",
        "svm_model = ovr.fit(train_data)\n",
        "\n",
        "# Make predictions\n",
        "predictions = svm_model.transform(test_data)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label_tactic_indexed\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg3xhMidNI-w",
        "outputId": "468d4a06-1fc9-4815-f6ea-a80851a13327"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9970414201183432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sparkContext.stop()"
      ],
      "metadata": {
        "id": "ciFnz0gyNKLG"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}