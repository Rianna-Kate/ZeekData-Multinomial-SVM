{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This implementation performs the following changes:\n",
        "\n",
        "1. Drops instances where any of the features are null or NaN.\n",
        "2. Uses a StringIndexer on the following features to convert them to numerical data. The feature name is changed to +'_indexed' when using this method.\n",
        "  a. 'service'\n",
        "  b. 'conn_state'\n",
        "  c. 'history'\n",
        "  d. 'proto'\n",
        "  e. 'dest_ip_zeek'\n",
        "  f. 'community_id'\n",
        "  g. 'uid'\n",
        "  h. 'src_ip_zeek'\n",
        "3. The original feature columns are removed.\n",
        "4. Drops any instances will null or NaN values.\n",
        "5. Uses a StringIndexer on the class labels, 'label_tactic', to convert them to numerical data.\n",
        "6. Uses a VectorAssembler on the new features.\n",
        "7. Trains the SVM model using OVR."
      ],
      "metadata": {
        "id": "jzV6s8jzYs7o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJvoGKb5BYEX"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Imports\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# ML Classifier Imports\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import OneVsRest\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder"
      ],
      "metadata": {
        "id": "8L5AnjDDBhPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"ce53\") \\\n",
        "    .master(\"local\") \\\n",
        "    .config(\"spark.driver.cores\", \"5\") \\\n",
        "    .config(\"spark.driver.memory\", \"10g\") \\\n",
        "    .config(\"spark.executor.memory\", \"5g\") \\\n",
        "    .config(\"spark.executor.cores\", \"4\") \\\n",
        "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", \"true\") \\\n",
        "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
        "    .config(\"spark.dynamicAllocation.minExecutors\", \"2\") \\\n",
        "    .config(\"spark.dynamicAllocation.maxExecutors\", \"4\") \\\n",
        "    .config(\"spark.executor.instances\", \"2\") \\\n",
        ".getOrCreate()"
      ],
      "metadata": {
        "id": "OKbaxu7jBogD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the parquet files (current example is 2 from the website)\n",
        "parquet_files = [\"/content/part-00000-1da06990-329c-4e38-913a-0f0aa39b388d-c000.snappy.parquet\", \"/content/part-00000-df678a79-4a73-452b-8e72-d624b2732f17-c000.snappy.parquet\"]\n",
        "\n",
        "# Read the parquet files into a dataframe\n",
        "df = spark.read.parquet(*parquet_files, inferSchema=True)"
      ],
      "metadata": {
        "id": "kG0MYVAqB7Ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5w6wHG7EFt9",
        "outputId": "2cecbe08-2540-401f-af8d-7bcb6f61d75c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- resp_pkts: integer (nullable = true)\n",
            " |-- service: string (nullable = true)\n",
            " |-- orig_ip_bytes: integer (nullable = true)\n",
            " |-- local_resp: boolean (nullable = true)\n",
            " |-- missed_bytes: integer (nullable = true)\n",
            " |-- proto: string (nullable = true)\n",
            " |-- duration: double (nullable = true)\n",
            " |-- conn_state: string (nullable = true)\n",
            " |-- dest_ip_zeek: string (nullable = true)\n",
            " |-- orig_pkts: integer (nullable = true)\n",
            " |-- community_id: string (nullable = true)\n",
            " |-- resp_ip_bytes: integer (nullable = true)\n",
            " |-- dest_port_zeek: integer (nullable = true)\n",
            " |-- orig_bytes: integer (nullable = true)\n",
            " |-- local_orig: boolean (nullable = true)\n",
            " |-- datetime: timestamp (nullable = true)\n",
            " |-- history: string (nullable = true)\n",
            " |-- resp_bytes: integer (nullable = true)\n",
            " |-- uid: string (nullable = true)\n",
            " |-- src_port_zeek: integer (nullable = true)\n",
            " |-- ts: double (nullable = true)\n",
            " |-- src_ip_zeek: string (nullable = true)\n",
            " |-- label_tactic: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the unique values in the df\n",
        "for col in df.columns:\n",
        "  unique_values = df.select(col).distinct().collect()\n",
        "  print(f\"Unique values for column '{col}': {unique_values}\")"
      ],
      "metadata": {
        "id": "PVdVw-mAIzW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove instances with null values\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "EEMTEgqlJWDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop the datetime column\n",
        "df = df.drop(\"datetime\")\n",
        "\n",
        "# Define columns to index\n",
        "columns_to_index = ['service', 'conn_state', 'history', 'proto', 'dest_ip_zeek', 'community_id', 'uid', 'src_ip_zeek', 'label_tactic']\n",
        "\n",
        "# Apply StringIndexer to each column\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_indexed\").fit(df) for column in columns_to_index]\n",
        "\n",
        "# Chain indexers together\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "\n",
        "# Fit and transform the data\n",
        "df_indexed = pipeline.fit(df).transform(df)\n",
        "\n",
        "# Drop original columns\n",
        "df_indexed = df_indexed.drop(*columns_to_index)\n",
        "\n",
        "# Drop rows with any null values\n",
        "df_indexed = df_indexed.dropna()\n",
        "\n",
        "# Show the schema of the DataFrame\n",
        "df_indexed.printSchema()"
      ],
      "metadata": {
        "id": "YheyBkMaEduN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df_indexed.columns:\n",
        "  unique_values = df_indexed.select(col).distinct().collect()\n",
        "  print(f\"Unique values for column '{col}': {unique_values}\")"
      ],
      "metadata": {
        "id": "rvcCCcKGOBjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of columns to assemble\n",
        "columns_to_assemble = df_indexed.columns\n",
        "\n",
        "# Remove the target column (label) if it's in the list\n",
        "columns_to_assemble.remove('label_tactic_indexed')\n",
        "\n",
        "# Create the VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=columns_to_assemble, outputCol=\"features\")\n",
        "\n",
        "# Transform the DataFrame\n",
        "df_assembled = assembler.transform(df_indexed)\n",
        "\n",
        "# Select only the features and label columns\n",
        "df_assembled = df_assembled.select(\"features\", \"label_tactic_indexed\")\n",
        "\n",
        "# Show the schema of the DataFrame\n",
        "df_assembled.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7fkbrdeEdqX",
        "outputId": "72081f15-b402-4a62-909c-f19097c5089a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- features: vector (nullable = true)\n",
            " |-- label_tactic_indexed: double (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test sets\n",
        "train_data, test_data = df_assembled.randomSplit([0.8, 0.2], seed=1)\n",
        "\n",
        "# Create the SVM model\n",
        "svm = LinearSVC(labelCol=\"label_tactic_indexed\", featuresCol=\"features\", maxIter=10)\n",
        "\n",
        "# One Vs. Rest\n",
        "ovr = OneVsRest(classifier=svm, labelCol='label_tactic_indexed')\n",
        "\n",
        "# Fit the model\n",
        "svm_model = ovr.fit(train_data)\n",
        "\n",
        "# Make predictions\n",
        "predictions = svm_model.transform(test_data)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label_tactic_indexed\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9EWiXPnEdnJ",
        "outputId": "438c001c-6dd2-4a27-eac8-5ad314f8e5ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9999774098086611\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes made:\n",
        "\n",
        "Used a StringIndexer on all columns (to include the classes) to change strings into numerical values.\n",
        "\n",
        "Dropped the datetime column.\n",
        "\n",
        "Ran OVR with SVM."
      ],
      "metadata": {
        "id": "NaUbYaFLRSRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sparkContext.stop()"
      ],
      "metadata": {
        "id": "L4KlhCB8Bz5Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}