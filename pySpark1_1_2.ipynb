{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This implementation performs the following changes:\n",
        "\n",
        "1. Converts 'datetime' timestamp datatype to string datatype 'datetime_str'\n",
        "> a. Currently not included in VectorAssembler due to overfitting\n",
        "\n",
        "2. Original feature columns implemented\n",
        "> a.  'local_resp', 'resp_ip_bytes', 'ts', 'datetime_str' currently not included in VectorAssembler due to overfitting\n",
        "\n"
      ],
      "metadata": {
        "id": "VdCaSxMhuID8"
      },
      "id": "VdCaSxMhuID8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc88b9ad-6ed7-462b-b241-048cab574a05",
      "metadata": {
        "id": "fc88b9ad-6ed7-462b-b241-048cab574a05"
      },
      "outputs": [],
      "source": [
        "!pip install numpy\n",
        "!pip install pyspark\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d6553129-3496-4321-a895-9b0a3311cd4c",
      "metadata": {
        "id": "d6553129-3496-4321-a895-9b0a3311cd4c"
      },
      "outputs": [],
      "source": [
        "#Basic Imports\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#ML Classifier Imports\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import OneVsRest\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import when\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "033bc9a6-1ff9-45d5-b6dd-eba9c159897d",
      "metadata": {
        "id": "033bc9a6-1ff9-45d5-b6dd-eba9c159897d"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"rka7\") \\\n",
        "    .master(\"local\") \\\n",
        "    .config(\"spark.driver.cores\", \"5\") \\\n",
        "    .config(\"spark.driver.memory\", \"10g\") \\\n",
        "    .config(\"spark.executor.memory\", \"5g\") \\\n",
        "    .config(\"spark.executor.cores\", \"4\") \\\n",
        "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", \"true\") \\\n",
        "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
        "    .config(\"spark.dynamicAllocation.minExecutors\", \"2\") \\\n",
        "    .config(\"spark.dynamicAllocation.maxExecutors\", \"4\") \\\n",
        "    .config(\"spark.executor.instances\", \"2\") \\\n",
        ".getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "75273b9a-33d4-4f17-b57c-44ee1fdcfda9",
      "metadata": {
        "id": "75273b9a-33d4-4f17-b57c-44ee1fdcfda9"
      },
      "outputs": [],
      "source": [
        "# Get the parquet files (current example is 2 from the website)\n",
        "parquet_files = [\"/content/part-00000-7c2e9adb-5430-4792-a42b-10ff5bbd46e8-c000.snappy.parquet\", \\\n",
        "                 \"/content/part-00000-df678a79-4a73-452b-8e72-d624b2732f17-c000.snappy.parquet\"]\n",
        "# Read the parquet files into a dataframe\n",
        "df = spark.read.parquet(*parquet_files, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove instances with null values\n",
        "df = df.dropna()\n",
        "#df.printSchema()"
      ],
      "metadata": {
        "id": "VZwf40RC5ab6"
      },
      "id": "VZwf40RC5ab6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert timestamp column to string\n",
        "#df = df.withColumn(\"datetime_str\", col(\"datetime\").cast(\"string\"))\n",
        "\n",
        "#Drop the datetime column\n",
        "df = df.drop(\"datetime\")\n",
        "\n",
        "# Define columns to index\n",
        "columns_to_index = ['service', 'conn_state', 'history', 'proto', 'dest_ip_zeek', 'community_id', 'uid', 'src_ip_zeek', 'label_tactic']\n",
        "\n",
        "# Apply StringIndexer to each column\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_indexed\").fit(df) for column in columns_to_index]\n",
        "\n",
        "# Chain indexers together\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "\n",
        "# Fit and transform the data\n",
        "df_indexed = pipeline.fit(df).transform(df)\n",
        "\n",
        "# Drop original columns\n",
        "df_indexed = df_indexed.drop(*columns_to_index)\n",
        "\n",
        "# Drop rows with any null values\n",
        "df_indexed = df_indexed.dropna()\n",
        "\n",
        "# Show the schema of the DataFrame\n",
        "#df_indexed.printSchema()\n",
        "#df.select(\"label_tactic\").distinct().collect()"
      ],
      "metadata": {
        "id": "HsgH0C-vCHq1"
      },
      "id": "HsgH0C-vCHq1",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_assembled = df_assembled.drop(\"datetime_str\")"
      ],
      "metadata": {
        "id": "vhnDU8vQemTg"
      },
      "id": "vhnDU8vQemTg",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of columns to assemble\n",
        "columns_to_assemble = df_indexed.columns\n",
        "\n",
        "# Include Numerical features to be assembled (Problems: 'local_resp', 'resp_ip_bytes', 'ts' is causing evaluations to overfit?)\n",
        "columns_to_assemble.extend(['resp_pkts', 'orig_ip_bytes', 'missed_bytes', 'duration', 'orig_pkts', \\\n",
        "               'dest_port_zeek', 'orig_bytes', 'local_orig', 'resp_bytes', 'src_port_zeek'])\n",
        "\n",
        "# Remove the target column (label) if it's in the list\n",
        "columns_to_assemble.remove('label_tactic_indexed')\n",
        "\n",
        "# Create the VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=columns_to_assemble, outputCol=\"features\")\n",
        "\n",
        "# Transform the DataFrame\n",
        "df_assembled = assembler.transform(df_indexed)\n",
        "\n",
        "# Select only the features and label columns\n",
        "df_assembled = df_assembled.select(\"features\", \"label_tactic_indexed\")\n",
        "\n",
        "# Show the schema of the DataFrame\n",
        "#df_assembled.printSchema()"
      ],
      "metadata": {
        "id": "XPtNBmnUCM3s"
      },
      "id": "XPtNBmnUCM3s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66981ef6-1c0b-42ce-965a-f60038c98d66",
      "metadata": {
        "id": "66981ef6-1c0b-42ce-965a-f60038c98d66"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and test sets\n",
        "train_data, test_data = df_assembled.randomSplit([0.8, 0.2], seed=1)\n",
        "\n",
        "# Create the SVM model\n",
        "svm = LinearSVC(labelCol=\"label_tactic_indexed\", featuresCol=\"features\", maxIter=10)\n",
        "\n",
        "# One Vs. Rest\n",
        "ovr = OneVsRest(classifier=svm, labelCol='label_tactic_indexed')\n",
        "\n",
        "# Fit the model\n",
        "svm_model = ovr.fit(train_data)\n",
        "\n",
        "# Make predictions\n",
        "predictions = svm_model.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Mulitple Evaluators for weighted precision, weighted recall, accuracy, weighted FPR\n",
        "eval_precision = MulticlassClassificationEvaluator(labelCol=\"label_tactic_indexed\", metricName=\"weightedPrecision\")\n",
        "eval_recall = MulticlassClassificationEvaluator(labelCol=\"label_tactic_indexed\", metricName=\"weightedRecall\")\n",
        "#eval_accuracy = MulticlassClassificationEvaluator(labelCol=\"label_tactic_indexed\", metricName=\"accuracy\")\n",
        "#eval_fpr = MulticlassClassificationEvaluator(labelCol=\"label_tactic_indexed\", metricName=\"weightedFalsePositiveRate\")\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = eval_accuracy.evaluate(predictions)\n",
        "print(\"Accuracy:\", evaluator)"
      ],
      "metadata": {
        "id": "W87jusSBx-gQ"
      },
      "id": "W87jusSBx-gQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes made:\n",
        "\n",
        "Numerical columns included in VectorAssembler.\n",
        "\n",
        "Datetime column converted to string for StringIndexer (Currently commented out because it is causing evaluations to overfit)"
      ],
      "metadata": {
        "id": "ISps2h1ztOPB"
      },
      "id": "ISps2h1ztOPB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problems:\n",
        "\n",
        "Vector assembler missing columns 'local_resp', 'resp_ip_bytes', 'ts', 'datetime_str' as it's causing evaluations to overfit?"
      ],
      "metadata": {
        "id": "wMkocUIcty__"
      },
      "id": "wMkocUIcty__"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35a12f58-f3b0-408c-904a-87018b432259",
      "metadata": {
        "id": "35a12f58-f3b0-408c-904a-87018b432259"
      },
      "outputs": [],
      "source": [
        "spark.sparkContext.stop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}